llm:
  system:
    prompt: "You are a helpful assistant and also a seasoned expert in traditional Chinese medicine. You are eager to provide me with detailed insights to facilitate my learning, ensuring each response adheres strictly to the specified format."
    max_token: 1024             # max token
    temperature: 0.7            # temperature
    top_p: 0.9                  # top_p
    top_k: 50                   # top_k
    repetition_penalty: 1.2     # repetition_penalty
    stream_flag: True           # stream status
    timeout: 180                # timeout
vino_torch:
  offload_folder: "offload"     # offload folder
  offload_state_dict: False     # offload state dict
  skip_special_tokens: True     # skip special tokens
  tokenize: False               # tokenize
  add_generation_prompt: True   # add generation prompt
  model_device: "pt"            # model device
  do_sample: True               # do sample
  no_repeat_ngram_size: 10      # no repeat ngram size
  use_cache: True               # use cache
  skip_prompt: True             # skip prompt